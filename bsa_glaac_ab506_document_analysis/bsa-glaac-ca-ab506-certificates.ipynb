{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "3a84231b-fce2-4d5b-b960-3fd275427d14"
            },
            "source": [
                "This notebook will let you analyze all certificate documents uploaded to [californiascouting.org](https://californiascouting.org) and query for the following information:\n",
                "\n",
                "|  |\n",
                "| --- |\n",
                "| This certificate is presented to who? |\n",
                "| For completing the California Child Abuse Mandated Reporter Online Training for what? |\n",
                "| Hours of education earned? |\n",
                "| What is the date of completion? |\n",
                "| What is the certificate number? |"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "42859db6-d9da-4b81-ac95-388475d7baed"
            },
            "source": [
                "# Create an S3 Bucket\n",
                "First create an S3 Bucket to store the documents to be analyzed. Go to [Amazon S3 > Buckets](https://s3.console.aws.amazon.com/s3/buckets?region=us-west-2) and create a bucket named `bsa-ca-ab506-training` in the `us-west-2` region.\n",
                "\n",
                "# Create an IAM Policy\n",
                "\n",
                "Go to the [IAM > Policies](https://us-east-1.console.aws.amazon.com/iamv2/home#/policies) page and create a Policy named `BSA_CA_AB-506_TextractAnalyzeDocuments` and grant it the following permissions:\n",
                "- Service: `S3`\n",
                "    - Actions\n",
                "        - List: `ListBucket`\n",
                "    - Resources\n",
                "        - Bucket name: `bsa-ca-ab506-training`\n",
                "- Service: `S3`\n",
                "    - Actions\n",
                "        - Read: `GetObject`\n",
                "        - Write: `PutObject`\n",
                "    - Resources\n",
                "        - Bucket name: `bsa-ca-ab506-training`\n",
                "        - Object name: `*` (any)\n",
                "- Service: `Textract`\n",
                "    - Actions\n",
                "        - Read: `AnalyzeDocument`\n",
                "        - Read: `GetDocumentAnalysis`\n",
                "        - Write: `StartDocumentAnalysis`\n",
                "\n",
                "# Set up to run this notebook\n",
                "\n",
                "Choose one of the two options for running this notebook:"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Option 1: Running this notebook in the cloud\n",
                "\n",
                "This option will run this notebook in a Notebook Instance in AWS SageMaker.\n",
                "\n",
                "### Step 1: Create an Amazon SageMaker Notebook Instance\n",
                "\n",
                "Follow instructions on https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html\n",
                "\n",
                "1. Under \"Notebook instance type\" choose \"ml.t3.medium\" or \"ml.t2.medium\" instance for the cheapest option.\n",
                "    1. T3 instance has Unlimited Burstable mode enabled automatically. T2 is also Burstable but it has Standard Mode enabled by default.\n",
                "    2. The price is roughly the same: \"ml.t3.medium\" is $0.05/hour and \"ml.t2.medium\" is $0.0464/hour.\n",
                "2. Under IAM Role choose \"Create a new role\". You will later attach your perviously-made policy to this new role.\n",
                "\n",
                "### Step 2: Attach your policy to the new AmazonSageMaker role\n",
                "\n",
                "1. Go to the [IAM > Roles](https://us-east-1.console.aws.amazon.com/iamv2/home#/roles) page and open the role starting with \"AmazonSageMaker\".\n",
                "2. In the \"Permissions policies\" list check the box for the new policy named `BSA_CA_AB-506_TextractAnalyzeDocuments`.\n",
                "3. Save the changes.\n",
                "\n",
                "### Step 3: Import this notebook into the Notebook Instance\n",
                "\n",
                "1. After the Notebook Instance starts click \"Open JupyterLab\" on the Notebook Instances list\n",
                "2. Click the upload button (up arrow button above the folder list) to upload this notebook.\n",
                "3. Now you can run the following code cells (skip \"Option 2\" steps)."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Option 2: Running this notebook on your computer\n",
                "\n",
                "This option will run this notebook on your computer with either Visual Studio Code or Azure Data Studio.\n",
                "\n",
                "### Step 1: Create an IAM User and Configure Boto3\n",
                "\n",
                "First you need to create an IAM User for Boto3 to use. This does not need to be unique per project so if you have already done before this you can leave AWS CLI / Boto3 configured to use that same User.\n",
                "\n",
                "Go to https://docs.aws.amazon.com/textract/latest/dg/setting-up.html and follow the steps to create the user.\n",
                "\n",
                "Next go to https://docs.aws.amazon.com/textract/latest/dg/setup-awscli-sdk.html and follow the steps to configure Boto3 with the credentials and default region of `us-west-2`.\n",
                "\n",
                "### Step 2: Create an IAM Role\n",
                "\n",
                "Go to the [IAM > Roles](https://us-east-1.console.aws.amazon.com/iamv2/home#/roles) page and create a Role. Follow these steps:\n",
                "\n",
                "#### Select trusted entity\n",
                "1. Select the \"Trusted entity type\" of `Custom trust policy`.\n",
                "2. On the right side under \"1. Add actions for STS\" leave the default `AssumeRole` option selected.\n",
                "3. On the right side under \"2. Add a principal\" click the Add button and choose:\n",
                "    - Principal type: `IAM users`\n",
                "    - ARN: Copy and paste the ARN from the User's details page. For example: `arn:aws:iam::840153288643:user/alex`\n",
                "4. Click Next.\n",
                "\n",
                "#### Add permissions\n",
                "\n",
                "1. In the \"Permissions policies\" list check the box for the new policy named `BSA_CA_AB-506_TextractAnalyzeDocuments`.\n",
                "2. Click Next.\n",
                "\n",
                "#### Role details\n",
                "\n",
                "1. Give the new role a name of `BSA_CA_AB-506_Textract_Role`.\n",
                "2. Create the role.\n",
                "3. Copy the ARN from the details page, for example: `arn:aws:iam::840153288643:role/BSA_CA_AB-506_Textract_Role`. You will need this when creating a Profile for Boto3 to use.\n",
                "\n",
                "### Step 3: Create a Profile in AWS CLI/SDK\n",
                "\n",
                "Follow the instructions on https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-cli.html to create a profile named `bsa-ca-ab506-textract` using the `role_arn` of `arn:aws:iam::840153288643:role/BSA_CA_AB-506_Textract_Role` that you copied in the previous step. These are the steps:\n",
                "\n",
                "1. Edit the `.aws/config` file in Linux or Mac, or the `C:\\Users\\USERNAME\\.aws\\config` file in Windows.\n",
                "2. Add the following:\n",
                "    ```\n",
                "    [profile bsa-ca-ab506-textract]\n",
                "        role_arn = arn:aws:iam::840153288643:role/BSA_CA_AB-506_Textract_Role\n",
                "        source_profile = default\n",
                "    ```\n",
                "3. Save the file.\n",
                "\n",
                "### Step 4: Use the new profile in this notebook\n",
                "\n",
                "Change `boto3.Session()` to `boto3.Session(profile_name='bsa-ca-ab506-textract')` in the first code block (not providing a profile name is meant for use with Amazon SageMaker).\n",
                "\n",
                "### Step 5: Install Python packages\n",
                "\n",
                "Install the following Python packages in your Notebook (in Azure Data Studio click on the \"Manage Packages\" icon at the top right of the notebook):\n",
                "\n",
                "- boto3\n",
                "- pandas\n",
                "- requests"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now you can run the following code cells:"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Script 1: Initialize packages and variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {
                "azdata_cell_guid": "29e5f3d2-74bf-431b-a74b-0880e0f17148",
                "language": "python",
                "tags": []
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import io\n",
                "from urllib.parse import urlparse\n",
                "\n",
                "import boto3\n",
                "import pandas as pd\n",
                "import pikepdf\n",
                "import requests\n",
                "from PyPDF2 import PdfReader, PdfWriter\n",
                "from PyPDF2.errors import PdfReadError\n",
                "\n",
                "input_csv_filename = \"bsa-glaac-ca-ab506-certificates-input2.csv\"\n",
                "output_csv_filename = \"bsa-glaac-ca-ab506-certificates.csv\"\n",
                "bucket_name = \"bsa-ca-ab506-training\"\n",
                "second_page_only_prefix = 'second_page_only/'\n",
                "\n",
                "# Change  the following `boto3.Session()` to `boto3.Session(profile_name='bsa-ca-ab506-textract')` if running this notebook locally\n",
                "session = boto3.Session()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Script 2: Upload all URLs to an S3 Bucket\n",
                "\n",
                "This script will take an input CSV (with the columns \"submit_id\" and \"upload_doc_url\") and upload the files to the S3 bucket.\n",
                "\n",
                "If you re-run this script block it will only upload the URLs remaining to be uploaded.\n",
                "\n",
                "If the file is a PDF and it has more than one page it will extract the 2nd page only and upload it to the path `second_page_only/`. Otherwise it will upload the file directly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {
                "azdata_cell_guid": "0c9e6b91-1e88-456c-bbee-4453a1f37aa4",
                "language": "python",
                "tags": []
            },
            "outputs": [],
            "source": [
                "def upload_from_url(url: str, bucket):\n",
                "    if url.startswith('https://californiascouting.org/'):\n",
                "        r = requests.get(url)\n",
                "        if r.status_code == 200:\n",
                "            # get file name\n",
                "            file_name = os.path.basename(urlparse(url).path)\n",
                "\n",
                "            # load file\n",
                "            raw_data = r.content\n",
                "\n",
                "            # detect pages\n",
                "            try:\n",
                "                reader = PdfReader(io.BytesIO(raw_data))\n",
                "                if len(reader.pages) > 1:\n",
                "                    # extracting the 2nd page\n",
                "                    # (Textract synchronous document analysis only works on one page PDFs)\n",
                "                    file_name = f\"{second_page_only_prefix}{file_name}\"\n",
                "                    page = reader.pages[1]\n",
                "                    writer = PdfWriter()\n",
                "                    writer.add_page(page)\n",
                "                    with io.BytesIO() as bytes_stream:\n",
                "                        writer.write(bytes_stream)\n",
                "                        bytes_stream.seek(0)\n",
                "                        bucket.upload_fileobj(bytes_stream, file_name)\n",
                "                else:\n",
                "                    # only one page PDF, saving the file directly to S3\n",
                "                    bucket.upload_fileobj(io.BytesIO(raw_data), file_name)\n",
                "            except PdfReadError:\n",
                "                # not a PDF, saving the file directly to S3\n",
                "                bucket.upload_fileobj(io.BytesIO(raw_data), file_name)\n",
                "            except Exception:\n",
                "                # PyPDF2 could not handle this file, switching to pikepdf\n",
                "                with pikepdf.Pdf.open(io.BytesIO(raw_data)) as pdf:\n",
                "                    if len(pdf.pages) > 1:\n",
                "                        # extracting the 2nd page\n",
                "                        # (Textract synchronous document analysis only works on one page PDFs)\n",
                "                        file_name = f\"{second_page_only_prefix}{file_name}\"\n",
                "                        page = pdf.pages[1]\n",
                "                        dst = pikepdf.Pdf.new()\n",
                "                        dst.pages.append(page)\n",
                "                        with io.BytesIO() as bytes_stream:\n",
                "                            dst.save(bytes_stream)\n",
                "                            bytes_stream.seek(0)\n",
                "                            bucket.upload_fileobj(bytes_stream, file_name)\n",
                "                    else:\n",
                "                        # only one page PDF, saving the file directly to S3\n",
                "                        bucket.upload_fileobj(io.BytesIO(raw_data), file_name)\n",
                "            print(f'Uploaded {file_name}')\n",
                "\n",
                "def upload_files(bucket_name: str, input_csv_filename: str, session: boto3.Session, testing=False) -> pd.DataFrame:\n",
                "    s3 = session.resource('s3')\n",
                "    bucket = s3.Bucket(bucket_name)\n",
                "\n",
                "    # load full URL list\n",
                "    df = pd.read_csv(input_csv_filename)\n",
                "    if testing:\n",
                "        df.sort_values(by=['upload_doc_url'], ascending=False, ignore_index=True, inplace=True)\n",
                "    if 'document' not in df:\n",
                "        df.insert(\n",
                "            loc=df.columns.get_loc('upload_doc_url') + 1,\n",
                "            column='document',\n",
                "            value=df['upload_doc_url'].apply(lambda x: os.path.basename(urlparse(x).path))\n",
                "        )\n",
                "\n",
                "    # check which have already been uploaded (we will filter these out when re-running this script)\n",
                "    objects = [obj.key.replace(second_page_only_prefix, '') for obj in bucket.objects.all()]\n",
                "    upload_df = df[~df['document'].isin(objects)]\n",
                "    if testing:\n",
                "        upload_df = upload_df.head(6)\n",
                "\n",
                "    # upload each URL to the bucket\n",
                "    object_count = len(upload_df.index)\n",
                "    for index, row in upload_df.iterrows():\n",
                "        print(f\"{index + 1} of {object_count}\")\n",
                "        upload_from_url(row['upload_doc_url'], bucket)\n",
                "    return df\n",
                "\n",
                "input_df = upload_files(bucket_name, input_csv_filename, session)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Script 3: Analyze all files in the S3 bucket using Amazon Textract\n",
                "\n",
                "If you re-run this script block it will only analyze the files remaining to be analyzed. If there are files that can't be analyzed it will print out a \"UnsupportedDocumentException\" warning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {
                "azdata_cell_guid": "218ee22f-e690-481a-80aa-7c5bce495fca",
                "language": "python",
                "tags": []
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1 of 19\n",
                        "UnsupportedDocumentException: 1644912340-upload-doc-Volunteer-BSA-online-training-cert.pdf\n",
                        "2 of 19\n",
                        "UnsupportedDocumentException: 1645690048-upload-doc-Certificate-00857117.pdf\n",
                        "3 of 19\n",
                        "UnsupportedDocumentException: 1645849648-upload-doc-Certificate-00859724.pdf\n",
                        "4 of 19\n",
                        "UnsupportedDocumentException: 1649096610-upload-doc-BackgroundCkAuthorization03312022.docx\n",
                        "5 of 19\n",
                        "UnsupportedDocumentException: 1651021311-upload-doc2-Background-Check-Consent-Form-2022-for-LiveScan.docx\n",
                        "6 of 19\n",
                        "UnsupportedDocumentException: 1652207203-upload-doc2-kara-scout-consent.pdf\n",
                        "7 of 19\n",
                        "UnsupportedDocumentException: 1653605339-upload-doc-2022-EXPLORER-MANDATED-REPORTING-CERT.docx\n",
                        "8 of 19\n",
                        "UnsupportedDocumentException: 1654449500-upload-doc-Mandated-Reporting-Certificate-2022_MEMB.docx\n",
                        "9 of 19\n",
                        "UnsupportedDocumentException: 1655096778-upload-doc2-IMG_5418.pdf\n",
                        "10 of 19\n",
                        "UnsupportedDocumentException: 1657064076-upload-doc2-Scouts-background-consent.pdf\n",
                        "11 of 19\n",
                        "UnsupportedDocumentException: 1658879476-upload-doc-Certificate-of-Completion-.docx\n",
                        "12 of 19\n",
                        "UnsupportedDocumentException: 1659164099-upload-doc2-Background-Consent-Form.docx\n",
                        "13 of 19\n",
                        "UnsupportedDocumentException: 1661229472-upload-doc-Certificate-01045040.pdf\n",
                        "14 of 19\n",
                        "UnsupportedDocumentException: 1664425468-upload-doc-Certificate-01091719.pdf\n",
                        "15 of 19\n",
                        "UnsupportedDocumentException: 1664475968-upload-doc2-Alex-Phan-Live-Scan.docx\n",
                        "16 of 19\n",
                        "UnsupportedDocumentException: 1664476754-upload-doc2-Alex-Phan-Live-Scan.docx\n",
                        "17 of 19\n",
                        "UnsupportedDocumentException: 1666328766-upload-doc-BSA-Consent-Form.pdf\n",
                        "18 of 19\n",
                        "UnsupportedDocumentException: 1668640933-upload-doc2-Troop713-background-check-permission-form.pdf\n",
                        "19 of 19\n",
                        "UnsupportedDocumentException: 1673156899-upload-doc-2023-Battad-Certificate-01184020.pdf\n"
                    ]
                }
            ],
            "source": [
                "def analyze_document(bucket_name: str, document: str, textract_client) -> dict:\n",
                "    try:\n",
                "        response = textract_client.analyze_document(\n",
                "            Document={'S3Object': {'Bucket': bucket_name, 'Name': document}},\n",
                "            FeatureTypes=[\"QUERIES\"],\n",
                "            QueriesConfig={'Queries': [\n",
                "                {'Text': 'This certificate is presented to who?', 'Alias': 'analyzed_certificate_name'},\n",
                "                {'Text': 'For completing the California Child Abuse Mandated Reporter Online Training for what?', 'Alias': 'analyzed_certificate_training_type'},\n",
                "                {'Text': 'Hours of education earned?', 'Alias': 'analyzed_certificate_hours'},\n",
                "                {'Text': 'What is the date of completion?', 'Alias': 'analyzed_certificate_date'},\n",
                "                {'Text': 'What is the certificate number?', 'Alias': 'analyzed_certificate_number'},\n",
                "                {'Text': 'What is the name (printed)?', 'Alias': 'analyzed_consent_name'},\n",
                "                {'Text': 'What is the scout member id?', 'Alias': 'analyzed_consent_scout_member_id'},\n",
                "                {'Text': 'What is the date?', 'Alias': 'analyzed_consent_date'}\n",
                "            ]}\n",
                "        )\n",
                "    except textract_client.exceptions.UnsupportedDocumentException:\n",
                "        print(f\"UnsupportedDocumentException: {document}\")\n",
                "        return\n",
                "\n",
                "    answers = {}\n",
                "\n",
                "    for block in response['Blocks']:\n",
                "        if block['BlockType'] == \"QUERY\" and \"Relationships\" in block:\n",
                "            answer_id = block['Relationships'][0]['Ids'][0]\n",
                "            if answer_id not in answers:\n",
                "                answers[answer_id] = {}\n",
                "            answers[answer_id]['alias'] = block['Query']['Alias']\n",
                "        elif block['BlockType'] == \"QUERY_RESULT\":\n",
                "            answer_id = block['Id']\n",
                "            if answer_id not in answers:\n",
                "                answers[answer_id] = {}\n",
                "            answers[answer_id]['answer'] = block['Text']\n",
                "            answers[answer_id]['confidence'] = block['Confidence']\n",
                "\n",
                "    row = {\n",
                "        'document': document\n",
                "    }\n",
                "    for answer in answers.values():\n",
                "        row[answer['alias']] = answer['answer']\n",
                "        row[f\"{answer['alias']}_confidence_score\"] = answer['confidence']\n",
                "\n",
                "    if row.get('analyzed_certificate_training_type'):\n",
                "        # is Certificate\n",
                "        for key in list(row.keys()):\n",
                "            if key.startswith('analyzed_consent'):\n",
                "                del row[key]\n",
                "    else:\n",
                "        # is Consent\n",
                "        for key in list(row.keys()):\n",
                "            if key.startswith('analyzed_certificate'):\n",
                "                del row[key]\n",
                "\n",
                "    return row\n",
                "\n",
                "def scan_bucket(bucket_name: str, csv_filename: str, session: boto3.Session, force_analyze=False, testing=False) -> pd.DataFrame:\n",
                "    s3 = session.resource('s3')\n",
                "    textract_client = session.client('textract', region_name='us-west-2')\n",
                "    bucket = s3.Bucket(bucket_name)\n",
                "    if testing:\n",
                "        objects = [obj.key for obj in bucket.objects.limit(4)]\n",
                "    else:\n",
                "        objects = [obj.key for obj in bucket.objects.all()]\n",
                "\n",
                "    # check which have already been analyzed (we will filter these out when re-running this script)\n",
                "    if not force_analyze and os.path.exists(csv_filename):\n",
                "        df = pd.read_csv(csv_filename)\n",
                "        if 'document' in df:\n",
                "            objects = list(filter(lambda key: key not in df['document'].values, objects))\n",
                "\n",
                "    object_count = len(objects)\n",
                "    for index, key in enumerate(objects):\n",
                "        print(f\"{index + 1} of {object_count}\")\n",
                "        row = analyze_document(bucket_name, key, textract_client)\n",
                "        if row:\n",
                "            columns = [\n",
                "                'document',\n",
                "                'analyzed_certificate_name',\n",
                "                'analyzed_certificate_name_confidence_score',\n",
                "                'analyzed_certificate_training_type',\n",
                "                'analyzed_certificate_training_type_confidence_score',\n",
                "                'analyzed_certificate_hours',\n",
                "                'analyzed_certificate_hours_confidence_score',\n",
                "                'analyzed_certificate_date',\n",
                "                'analyzed_certificate_date_confidence_score',\n",
                "                'analyzed_certificate_number',\n",
                "                'analyzed_certificate_number_confidence_score',\n",
                "                'analyzed_consent_name',\n",
                "                'analyzed_consent_name_confidence_score',\n",
                "                'analyzed_consent_scout_member_id',\n",
                "                'analyzed_consent_scout_member_id_confidence_score',\n",
                "                'analyzed_consent_date',\n",
                "                'analyzed_consent_date_confidence_score'   \n",
                "            ]\n",
                "            row_df = pd.DataFrame([row], columns=columns)\n",
                "            if not os.path.exists(csv_filename):\n",
                "                row_df.to_csv(csv_filename, index=False)\n",
                "            else:\n",
                "                row_df.to_csv(csv_filename, mode='a', index=False, header=False)\n",
                "            print(row)\n",
                "    df = pd.read_csv(csv_filename)\n",
                "    return df\n",
                "\n",
                "output_df = scan_bucket(bucket_name, output_csv_filename, session)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Script 4: Merge the input and output together\n",
                "\n",
                "If the output does not include \"submit_id\" column, merge the input and output data frames and save it to the output CSV file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'submit_id' not in output_df:\n",
                "    output_df['document'] = output_df['document'].apply(lambda x: x.replace(second_page_only_prefix, ''))\n",
                "    merged_df = pd.merge(input_df, output_df, on=\"document\")\n",
                "    merged_df.to_csv(output_csv_filename.replace('.csv', '_merged.csv'), index=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
